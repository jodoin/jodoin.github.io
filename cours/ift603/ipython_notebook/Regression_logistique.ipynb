{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Régression logistique\n",
    "\n",
    "Petit code illustrant l'optimisation d'un régresseur logistique 2 classes par descente de gradient par *BATCH*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "# Génération des données d'entraînement\n",
    "X_1 = 0.5*np.random.randn(10,2) + np.array([[5,1]]) # Gaussienne centrée en mu_1=[5,1]\n",
    "t_1 = np.ones(10)\n",
    "X_2 = 0.5*np.random.randn(10,2) + np.array([[2,3]]) # Gaussienne centrée en mu_2=[2,3]\n",
    "t_2 = np.zeros(10)\n",
    "\n",
    "# Fusionne toutes les données dans un seul ensemble d'entraînement\n",
    "X = np.vstack([X_1,X_2])\n",
    "t = np.hstack([t_1,t_2])\n",
    "\n",
    "#Mélange dans un ordre aléatoire\n",
    "p = np.random.permutation(20)     \n",
    "X = X[p,:]\n",
    "t = t[p]\n",
    "\n",
    "def sigm(a):\n",
    "    return 1./(1. + np.exp(-a))\n",
    "\n",
    "# Pour visualiser le classifieur\n",
    "delta = 0.025\n",
    "ix = np.arange(1.0, 6.0, delta)\n",
    "iy = np.arange(0.0, 4.0, delta)\n",
    "iX, iY = np.meshgrid(ix, iy)\n",
    "\n",
    "# Initialisation du classifieur de la régression logistique\n",
    "# (ces valeurs sont choisies pour rendre l'illustration plus intéressante...\n",
    "#  normalement on initialiserait à 0.)\n",
    "w = np.array([1.,2.])\n",
    "w_0 = -5.\n",
    "eta = 0.5 # Normalement, on utiliserait une valeur plus petite\n",
    "\n",
    "# Descente de gradient de type \"BATCH\" pour la régression logistique\n",
    "pyplot.subplots(figsize=(20,25))\n",
    "for it in range(20):\n",
    "    # Calcul p(C_1 | x)\n",
    "    o = sigm(w_0 + np.dot(X,w))\n",
    "    \n",
    "    # Calcul les gradients de w et w_0\n",
    "    grad_w = np.dot((o-t),X)\n",
    "    grad_w_0 = np.sum(o-t)\n",
    "    \n",
    "    # Mise à jour des paramètres\n",
    "    w -= eta/20. * grad_w\n",
    "    w_0 -= eta/20. * grad_w_0\n",
    "    \n",
    "    # Visualisation\n",
    "    pyplot.subplot(5,4,1+it)\n",
    "    dummy = pyplot.scatter(X_1[:,0],X_1[:,1])\n",
    "    dummy = pyplot.scatter(X_2[:,0],X_2[:,1],marker='+')\n",
    "\n",
    "    contour_out = sigm(np.dot(np.hstack([iX.reshape((-1,1)),iY.reshape((-1,1))]),w)+w_0).reshape(iX.shape)\n",
    "    dummy = pyplot.contour(iX,iY,contour_out,[0.5], colors='k')\n",
    "    pyplot.clabel(dummy, inline=1, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
